{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1b8dada-ef50-4370-a3c8-98bf905d23d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MEMBER 1: DATA PREPROCESSING AND TRADITIONAL ML MODELS\n",
      "================================================================================\n",
      "\n",
      "--- PHASE 1: DATA LOADING ---\n",
      "Dataset loaded successfully: 297 rows, 14 columns\n",
      "\n",
      "Column names: ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'condition']\n",
      "\n",
      "First 5 rows of the dataset:\n",
      "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
      "0   69    1   0       160   234    1        2      131      0      0.1      1   \n",
      "1   69    0   0       140   239    0        0      151      0      1.8      0   \n",
      "2   66    0   0       150   226    0        0      114      0      2.6      2   \n",
      "3   65    1   0       138   282    1        2      174      0      1.4      1   \n",
      "4   64    1   0       110   211    0        2      144      1      1.8      1   \n",
      "\n",
      "   ca  thal  condition  \n",
      "0   1     0          0  \n",
      "1   2     0          0  \n",
      "2   0     0          0  \n",
      "3   1     0          1  \n",
      "4   0     0          0  \n",
      "\n",
      "--- PHASE 2: EXPLORATORY DATA ANALYSIS ---\n",
      "\n",
      "Dataset Statistical Summary:\n",
      "              age         sex          cp    trestbps        chol         fbs  \\\n",
      "count  297.000000  297.000000  297.000000  297.000000  297.000000  297.000000   \n",
      "mean    54.542088    0.676768    2.158249  131.693603  247.350168    0.144781   \n",
      "std      9.049736    0.468500    0.964859   17.762806   51.997583    0.352474   \n",
      "min     29.000000    0.000000    0.000000   94.000000  126.000000    0.000000   \n",
      "25%     48.000000    0.000000    2.000000  120.000000  211.000000    0.000000   \n",
      "50%     56.000000    1.000000    2.000000  130.000000  243.000000    0.000000   \n",
      "75%     61.000000    1.000000    3.000000  140.000000  276.000000    0.000000   \n",
      "max     77.000000    1.000000    3.000000  200.000000  564.000000    1.000000   \n",
      "\n",
      "          restecg     thalach       exang     oldpeak       slope          ca  \\\n",
      "count  297.000000  297.000000  297.000000  297.000000  297.000000  297.000000   \n",
      "mean     0.996633  149.599327    0.326599    1.055556    0.602694    0.676768   \n",
      "std      0.994914   22.941562    0.469761    1.166123    0.618187    0.938965   \n",
      "min      0.000000   71.000000    0.000000    0.000000    0.000000    0.000000   \n",
      "25%      0.000000  133.000000    0.000000    0.000000    0.000000    0.000000   \n",
      "50%      1.000000  153.000000    0.000000    0.800000    1.000000    0.000000   \n",
      "75%      2.000000  166.000000    1.000000    1.600000    1.000000    1.000000   \n",
      "max      2.000000  202.000000    1.000000    6.200000    2.000000    3.000000   \n",
      "\n",
      "             thal   condition  \n",
      "count  297.000000  297.000000  \n",
      "mean     0.835017    0.461279  \n",
      "std      0.956690    0.499340  \n",
      "min      0.000000    0.000000  \n",
      "25%      0.000000    0.000000  \n",
      "50%      0.000000    0.000000  \n",
      "75%      2.000000    1.000000  \n",
      "max      2.000000    1.000000  \n",
      "\n",
      "Missing Values Count:\n",
      "age          0\n",
      "sex          0\n",
      "cp           0\n",
      "trestbps     0\n",
      "chol         0\n",
      "fbs          0\n",
      "restecg      0\n",
      "thalach      0\n",
      "exang        0\n",
      "oldpeak      0\n",
      "slope        0\n",
      "ca           0\n",
      "thal         0\n",
      "condition    0\n",
      "dtype: int64\n",
      "\n",
      "Data Types:\n",
      "age            int64\n",
      "sex            int64\n",
      "cp             int64\n",
      "trestbps       int64\n",
      "chol           int64\n",
      "fbs            int64\n",
      "restecg        int64\n",
      "thalach        int64\n",
      "exang          int64\n",
      "oldpeak      float64\n",
      "slope          int64\n",
      "ca             int64\n",
      "thal           int64\n",
      "condition      int64\n",
      "dtype: object\n",
      "\n",
      "Target column identified: condition\n",
      "\n",
      "Class Distribution:\n",
      "condition\n",
      "0    160\n",
      "1    137\n",
      "Name: count, dtype: int64\n",
      "Class distribution plot saved\n",
      "Correlation heatmap saved\n",
      "Feature distribution histograms saved\n",
      "\n",
      "--- PHASE 3: DATA PREPROCESSING ---\n",
      "Numeric features (13): ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal']\n",
      "Categorical features (0): []\n",
      "\n",
      "Training set size: 237 samples\n",
      "Testing set size: 60 samples\n",
      "\n",
      " Preprocessor saved to artifacts/models/preprocessor.pkl\n",
      "Train-test split saved to artifacts/models/train_test_split.pkl\n",
      "\n",
      "--- PHASE 4: DECISION TREE MODEL ---\n",
      "\n",
      "Decision Tree Performance:\n",
      "  Accuracy:  0.8000\n",
      "  Precision: 0.8333\n",
      "  Recall:    0.7143\n",
      "  F1-Score:  0.7692\n",
      "\n",
      "Performing 5-Fold Cross-Validation for Decision Tree...\n",
      "CV Accuracy Scores: [0.64583333 0.72916667 0.65957447 0.85106383 0.74468085]\n",
      "Mean CV Accuracy: 0.7261 (+/- 0.1465)\n",
      "Decision Tree confusion matrix saved\n",
      "Decision Tree ROC curve saved\n",
      "Decision Tree PR curve saved\n",
      "Decision Tree model and metrics saved\n",
      "\n",
      "--- PHASE 5: RANDOM FOREST MODEL ---\n",
      "\n",
      "Random Forest Performance:\n",
      "  Accuracy:  0.8333\n",
      "  Precision: 0.8750\n",
      "  Recall:    0.7500\n",
      "  F1-Score:  0.8077\n",
      "\n",
      "Performing 5-Fold Cross-Validation for Random Forest...\n",
      "CV Accuracy Scores: [0.75       0.77083333 0.85106383 0.89361702 0.82978723]\n",
      "Mean CV Accuracy: 0.8191 (+/- 0.1050)\n",
      "Random Forest feature importance plot saved\n",
      "Random Forest confusion matrix saved\n",
      "Random Forest ROC curve saved\n",
      "Random Forest PR curve saved\n",
      "Random Forest model and metrics saved\n",
      "\n",
      "================================================================================\n",
      "MEMBER 1 COMPLETION SUMMARY\n",
      "================================================================================\n",
      "\n",
      " Data preprocessing completed\n",
      "Decision Tree trained and evaluated\n",
      "Random Forest trained and evaluated\n",
      "All models saved to artifacts/models/\n",
      "All metrics saved to artifacts/metrics/\n",
      "All figures saved to artifacts/figures/\n",
      "\n",
      "Ready for Member 2 to proceed with Naive Bayes and Deep Learning models.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Member 1: Data Preprocessing and Traditional Machine Learning Models\n",
    "Heart Disease Classification using Decision Tree and Random Forest\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 1: IMPORT LIBRARIES AND SETUP\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import numpy as np  # For numerical operations and array handling\n",
    "import matplotlib.pyplot as plt  # For creating visualizations\n",
    "import seaborn as sns  # For advanced statistical visualizations\n",
    "from sklearn.model_selection import train_test_split, cross_val_score  # For data splitting and cross-validation\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder  # For feature scaling and encoding\n",
    "from sklearn.compose import ColumnTransformer  # For applying different preprocessing to different columns\n",
    "from sklearn.pipeline import Pipeline  # For creating preprocessing pipelines\n",
    "from sklearn.tree import DecisionTreeClassifier  # For decision tree model\n",
    "from sklearn.ensemble import RandomForestClassifier  # For random forest ensemble model\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score  # For model evaluation metrics\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_recall_curve  # For advanced evaluation\n",
    "import joblib  # For saving models and preprocessors\n",
    "import json  # For saving metrics as JSON files\n",
    "import os  # For directory and file operations\n",
    "import warnings  # For suppressing unnecessary warnings\n",
    "warnings.filterwarnings('ignore')  # Suppress warnings for cleaner output\n",
    "\n",
    "# Set random seed for reproducibility across all random operations\n",
    "np.random.seed(42)  # Ensures consistent results across multiple runs\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 2: CREATE REQUIRED DIRECTORIES\n",
    "# ============================================================================\n",
    "\n",
    "os.makedirs('artifacts/models', exist_ok=True)  # Create models directory if it doesn't exist\n",
    "os.makedirs('artifacts/metrics', exist_ok=True)  # Create metrics directory if it doesn't exist\n",
    "os.makedirs('artifacts/figures', exist_ok=True)  # Create figures directory if it doesn't exist\n",
    "\n",
    "print(\"=\"*80)  # Print separator line for visual clarity\n",
    "print(\"MEMBER 1: DATA PREPROCESSING AND TRADITIONAL ML MODELS\")  # Print module header\n",
    "print(\"=\"*80)  # Print separator line for visual clarity\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 3: DATA LOADING AND INITIAL EXPLORATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n--- PHASE 1: DATA LOADING ---\")  # Announce data loading phase\n",
    "df = pd.read_csv('data/heart_cleveland_upload.csv')  # Load heart disease dataset from CSV file\n",
    "\n",
    "# Clean column names by removing spaces and special characters\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')  # Standardize column names to lowercase with underscores\n",
    "\n",
    "print(f\"Dataset loaded successfully: {df.shape[0]} rows, {df.shape[1]} columns\")  # Display dataset dimensions\n",
    "print(f\"\\nColumn names: {list(df.columns)}\")  # Display all column names\n",
    "\n",
    "# Display first few rows to understand data structure\n",
    "print(\"\\nFirst 5 rows of the dataset:\")  # Announce data preview\n",
    "print(df.head())  # Show first 5 rows of data\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 4: DATA DESCRIPTION AND EXPLORATORY DATA ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n--- PHASE 2: EXPLORATORY DATA ANALYSIS ---\")  # Announce EDA phase\n",
    "\n",
    "# Generate comprehensive statistical summary of dataset\n",
    "print(\"\\nDataset Statistical Summary:\")  # Announce statistical summary\n",
    "print(df.describe())  # Display mean, std, min, max, quartiles for numeric columns\n",
    "\n",
    "# Check for missing values in dataset\n",
    "print(\"\\nMissing Values Count:\")  # Announce missing values check\n",
    "print(df.isnull().sum())  # Display count of missing values per column\n",
    "\n",
    "# Identify data types of each column\n",
    "print(\"\\nData Types:\")  # Announce data types information\n",
    "print(df.dtypes)  # Display data type of each column\n",
    "\n",
    "# Prepare target variable (assuming last column or 'target'/'condition' column)\n",
    "target_column = None  # Initialize target column variable\n",
    "if 'target' in df.columns:  # Check if 'target' column exists\n",
    "    target_column = 'target'  # Set target column name\n",
    "elif 'condition' in df.columns:  # Check if 'condition' column exists\n",
    "    target_column = 'condition'  # Set target column name\n",
    "else:  # If neither common name exists\n",
    "    target_column = df.columns[-1]  # Use last column as target\n",
    "\n",
    "print(f\"\\nTarget column identified: {target_column}\")  # Display target column name\n",
    "\n",
    "# Analyze class distribution in target variable\n",
    "print(\"\\nClass Distribution:\")  # Announce class distribution analysis\n",
    "print(df[target_column].value_counts())  # Count occurrences of each class\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 2A: VISUALIZE CLASS BALANCE\n",
    "# ============================================================================\n",
    "\n",
    "plt.figure(figsize=(8, 6))  # Create figure with specified size\n",
    "df[target_column].value_counts().plot(kind='bar', color=['#2E86AB', '#A23B72'])  # Create bar plot of class counts\n",
    "plt.title('Class Distribution - Heart Disease', fontsize=14, fontweight='bold')  # Add title to plot\n",
    "plt.xlabel('Class (0=No Disease, 1=Disease)', fontsize=12)  # Label x-axis\n",
    "plt.ylabel('Frequency', fontsize=12)  # Label y-axis\n",
    "plt.xticks(rotation=0)  # Keep x-axis labels horizontal\n",
    "plt.grid(axis='y', alpha=0.3)  # Add horizontal grid lines\n",
    "plt.tight_layout()  # Adjust layout to prevent label cutoff\n",
    "plt.savefig('artifacts/figures/class_distribution.png', dpi=300, bbox_inches='tight')  # Save figure to file\n",
    "plt.close()  # Close figure to free memory\n",
    "print(\"Class distribution plot saved\")  # Confirm plot saved\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 2B: CORRELATION HEATMAP\n",
    "# ============================================================================\n",
    "\n",
    "plt.figure(figsize=(12, 10))  # Create large figure for correlation matrix\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns  # Select only numeric columns\n",
    "correlation_matrix = df[numeric_cols].corr()  # Calculate correlation matrix\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,  # Create annotated heatmap\n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})  # Set heatmap properties\n",
    "plt.title('Feature Correlation Heatmap', fontsize=14, fontweight='bold')  # Add title\n",
    "plt.tight_layout()  # Adjust layout\n",
    "plt.savefig('artifacts/figures/correlation_heatmap.png', dpi=300, bbox_inches='tight')  # Save figure\n",
    "plt.close()  # Close figure\n",
    "print(\"Correlation heatmap saved\")  # Confirm plot saved\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 2C: FEATURE DISTRIBUTIONS\n",
    "# ============================================================================\n",
    "\n",
    "# Plot histograms for all numeric features\n",
    "fig, axes = plt.subplots(4, 4, figsize=(16, 12))  # Create 4x4 subplot grid\n",
    "axes = axes.ravel()  # Flatten axes array for easier iteration\n",
    "for idx, col in enumerate(numeric_cols[:16]):  # Iterate through first 16 numeric columns\n",
    "    if idx < len(axes):  # Check if subplot exists\n",
    "        axes[idx].hist(df[col].dropna(), bins=30, color='skyblue', edgecolor='black', alpha=0.7)  # Create histogram\n",
    "        axes[idx].set_title(f'{col}', fontsize=10, fontweight='bold')  # Add column name as title\n",
    "        axes[idx].set_xlabel('Value', fontsize=8)  # Label x-axis\n",
    "        axes[idx].set_ylabel('Frequency', fontsize=8)  # Label y-axis\n",
    "        axes[idx].grid(axis='y', alpha=0.3)  # Add grid lines\n",
    "for idx in range(len(numeric_cols), len(axes)):  # Remove empty subplots\n",
    "    fig.delaxes(axes[idx])  # Delete unused subplot\n",
    "plt.tight_layout()  # Adjust spacing between subplots\n",
    "plt.savefig('artifacts/figures/feature_distributions.png', dpi=300, bbox_inches='tight')  # Save figure\n",
    "plt.close()  # Close figure\n",
    "print(\"Feature distribution histograms saved\")  # Confirm plot saved\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 5: DATA PREPROCESSING AND FEATURE ENGINEERING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n--- PHASE 3: DATA PREPROCESSING ---\")  # Announce preprocessing phase\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop(columns=[target_column])  # Remove target column to get features\n",
    "y = df[target_column]  # Extract target column\n",
    "\n",
    "# Identify numeric and categorical columns for preprocessing\n",
    "numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()  # Get list of numeric feature names\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()  # Get list of categorical feature names\n",
    "\n",
    "print(f\"Numeric features ({len(numeric_features)}): {numeric_features}\")  # Display numeric features\n",
    "print(f\"Categorical features ({len(categorical_features)}): {categorical_features}\")  # Display categorical features\n",
    "\n",
    "# Create preprocessing pipeline for numeric features\n",
    "numeric_transformer = Pipeline(steps=[  # Define numeric preprocessing steps\n",
    "    ('scaler', StandardScaler())  # Apply standard scaling (mean=0, std=1)\n",
    "])\n",
    "\n",
    "# If categorical features exist, encode them\n",
    "if len(categorical_features) > 0:  # Check if categorical features present\n",
    "    from sklearn.preprocessing import OneHotEncoder  # Import one-hot encoder\n",
    "    categorical_transformer = Pipeline(steps=[  # Define categorical preprocessing steps\n",
    "        ('onehot', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))  # One-hot encode\n",
    "    ])\n",
    "    preprocessor = ColumnTransformer(  # Combine transformers for different column types\n",
    "        transformers=[  # List of transformers\n",
    "            ('num', numeric_transformer, numeric_features),  # Apply numeric transformer to numeric features\n",
    "            ('cat', categorical_transformer, categorical_features)  # Apply categorical transformer to categorical features\n",
    "        ])\n",
    "else:  # If no categorical features\n",
    "    preprocessor = ColumnTransformer(  # Create transformer for numeric features only\n",
    "        transformers=[  # List of transformers\n",
    "            ('num', numeric_transformer, numeric_features)  # Apply numeric transformer\n",
    "        ])\n",
    "\n",
    "# Split data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)  # Split with stratification\n",
    "\n",
    "print(f\"\\nTraining set size: {X_train.shape[0]} samples\")  # Display training set size\n",
    "print(f\"Testing set size: {X_test.shape[0]} samples\")  # Display testing set size\n",
    "\n",
    "# Fit preprocessor on training data and transform both sets\n",
    "X_train_processed = preprocessor.fit_transform(X_train)  # Fit on training data and transform\n",
    "X_test_processed = preprocessor.transform(X_test)  # Transform test data using fitted preprocessor\n",
    "\n",
    "# Save preprocessor for use by other team members\n",
    "joblib.dump(preprocessor, 'artifacts/models/preprocessor.pkl')  # Save preprocessor to file\n",
    "print(\"\\n Preprocessor saved to artifacts/models/preprocessor.pkl\")  # Confirm save\n",
    "\n",
    "# Save train-test split for consistency across models\n",
    "joblib.dump((X_train, X_test, y_train, y_test), 'artifacts/models/train_test_split.pkl')  # Save split data\n",
    "print(\"Train-test split saved to artifacts/models/train_test_split.pkl\")  # Confirm save\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 6: DECISION TREE MODEL\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n--- PHASE 4: DECISION TREE MODEL ---\")  # Announce decision tree phase\n",
    "\n",
    "# Initialize and train Decision Tree classifier\n",
    "dt_model = DecisionTreeClassifier(random_state=42, max_depth=10, min_samples_split=10)  # Create DT with hyperparameters\n",
    "dt_model.fit(X_train_processed, y_train)  # Train model on processed training data\n",
    "y_pred_dt = dt_model.predict(X_test_processed)  # Make predictions on test set\n",
    "y_pred_proba_dt = dt_model.predict_proba(X_test_processed)[:, 1]  # Get probability predictions for positive class\n",
    "\n",
    "# Calculate evaluation metrics for Decision Tree\n",
    "dt_accuracy = accuracy_score(y_test, y_pred_dt)  # Calculate accuracy\n",
    "dt_precision = precision_score(y_test, y_pred_dt, average='binary', zero_division=0)  # Calculate precision\n",
    "dt_recall = recall_score(y_test, y_pred_dt, average='binary', zero_division=0)  # Calculate recall\n",
    "dt_f1 = f1_score(y_test, y_pred_dt, average='binary', zero_division=0)  # Calculate F1-score\n",
    "\n",
    "print(f\"\\nDecision Tree Performance:\")  # Announce DT results\n",
    "print(f\"  Accuracy:  {dt_accuracy:.4f}\")  # Display accuracy\n",
    "print(f\"  Precision: {dt_precision:.4f}\")  # Display precision\n",
    "print(f\"  Recall:    {dt_recall:.4f}\")  # Display recall\n",
    "print(f\"  F1-Score:  {dt_f1:.4f}\")  # Display F1-score\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 4A: CROSS-VALIDATION FOR DECISION TREE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nPerforming 5-Fold Cross-Validation for Decision Tree...\")  # Announce CV\n",
    "dt_cv_scores = cross_val_score(dt_model, X_train_processed, y_train, cv=5, scoring='accuracy')  # Perform 5-fold CV\n",
    "print(f\"CV Accuracy Scores: {dt_cv_scores}\")  # Display individual fold scores\n",
    "print(f\"Mean CV Accuracy: {dt_cv_scores.mean():.4f} (+/- {dt_cv_scores.std() * 2:.4f})\")  # Display mean and std\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 4B: CONFUSION MATRIX FOR DECISION TREE\n",
    "# ============================================================================\n",
    "\n",
    "dt_cm = confusion_matrix(y_test, y_pred_dt)  # Calculate confusion matrix\n",
    "plt.figure(figsize=(8, 6))  # Create figure\n",
    "sns.heatmap(dt_cm, annot=True, fmt='d', cmap='Blues', cbar=True,  # Create heatmap\n",
    "            xticklabels=['No Disease', 'Disease'],  # Label x-axis\n",
    "            yticklabels=['No Disease', 'Disease'])  # Label y-axis\n",
    "plt.title('Decision Tree - Confusion Matrix', fontsize=14, fontweight='bold')  # Add title\n",
    "plt.ylabel('Actual', fontsize=12)  # Label y-axis\n",
    "plt.xlabel('Predicted', fontsize=12)  # Label x-axis\n",
    "plt.tight_layout()  # Adjust layout\n",
    "plt.savefig('artifacts/figures/dt_confusion_matrix.png', dpi=300, bbox_inches='tight')  # Save figure\n",
    "plt.close()  # Close figure\n",
    "print(\"Decision Tree confusion matrix saved\")  # Confirm save\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 4C: ROC CURVE FOR DECISION TREE\n",
    "# ============================================================================\n",
    "\n",
    "fpr_dt, tpr_dt, _ = roc_curve(y_test, y_pred_proba_dt)  # Calculate ROC curve points\n",
    "roc_auc_dt = auc(fpr_dt, tpr_dt)  # Calculate area under ROC curve\n",
    "\n",
    "plt.figure(figsize=(8, 6))  # Create figure\n",
    "plt.plot(fpr_dt, tpr_dt, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc_dt:.2f})')  # Plot ROC curve\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')  # Plot diagonal reference\n",
    "plt.xlim([0.0, 1.0])  # Set x-axis limits\n",
    "plt.ylim([0.0, 1.05])  # Set y-axis limits\n",
    "plt.xlabel('False Positive Rate', fontsize=12)  # Label x-axis\n",
    "plt.ylabel('True Positive Rate', fontsize=12)  # Label y-axis\n",
    "plt.title('Decision Tree - ROC Curve', fontsize=14, fontweight='bold')  # Add title\n",
    "plt.legend(loc=\"lower right\")  # Add legend\n",
    "plt.grid(alpha=0.3)  # Add grid\n",
    "plt.tight_layout()  # Adjust layout\n",
    "plt.savefig('artifacts/figures/dt_roc_curve.png', dpi=300, bbox_inches='tight')  # Save figure\n",
    "plt.close()  # Close figure\n",
    "print(\"Decision Tree ROC curve saved\")  # Confirm save\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 4D: PRECISION-RECALL CURVE FOR DECISION TREE\n",
    "# ============================================================================\n",
    "\n",
    "precision_dt, recall_dt, _ = precision_recall_curve(y_test, y_pred_proba_dt)  # Calculate PR curve points\n",
    "plt.figure(figsize=(8, 6))  # Create figure\n",
    "plt.plot(recall_dt, precision_dt, color='blue', lw=2, label='PR curve')  # Plot PR curve\n",
    "plt.xlabel('Recall', fontsize=12)  # Label x-axis\n",
    "plt.ylabel('Precision', fontsize=12)  # Label y-axis\n",
    "plt.title('Decision Tree - Precision-Recall Curve', fontsize=14, fontweight='bold')  # Add title\n",
    "plt.legend(loc=\"lower left\")  # Add legend\n",
    "plt.grid(alpha=0.3)  # Add grid\n",
    "plt.tight_layout()  # Adjust layout\n",
    "plt.savefig('artifacts/figures/dt_pr_curve.png', dpi=300, bbox_inches='tight')  # Save figure\n",
    "plt.close()  # Close figure\n",
    "print(\"Decision Tree PR curve saved\")  # Confirm save\n",
    "\n",
    "# Save Decision Tree model and metrics\n",
    "joblib.dump(dt_model, 'artifacts/models/decision_tree_model.pkl')  # Save trained model\n",
    "dt_metrics = {  # Create metrics dictionary\n",
    "    'model': 'Decision Tree',  # Model name\n",
    "    'accuracy': float(dt_accuracy),  # Convert to float for JSON serialization\n",
    "    'precision': float(dt_precision),  # Convert to float\n",
    "    'recall': float(dt_recall),  # Convert to float\n",
    "    'f1_score': float(dt_f1),  # Convert to float\n",
    "    'roc_auc': float(roc_auc_dt),  # Convert to float\n",
    "    'cv_mean': float(dt_cv_scores.mean()),  # Cross-validation mean\n",
    "    'cv_std': float(dt_cv_scores.std())  # Cross-validation standard deviation\n",
    "}\n",
    "with open('artifacts/metrics/decision_tree_metrics.json', 'w') as f:  # Open file for writing\n",
    "    json.dump(dt_metrics, f, indent=4)  # Save metrics as formatted JSON\n",
    "print(\"Decision Tree model and metrics saved\")  # Confirm save\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 7: RANDOM FOREST MODEL\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n--- PHASE 5: RANDOM FOREST MODEL ---\")  # Announce random forest phase\n",
    "\n",
    "# Initialize and train Random Forest classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10, min_samples_split=10)  # Create RF\n",
    "rf_model.fit(X_train_processed, y_train)  # Train model on processed training data\n",
    "y_pred_rf = rf_model.predict(X_test_processed)  # Make predictions on test set\n",
    "y_pred_proba_rf = rf_model.predict_proba(X_test_processed)[:, 1]  # Get probability predictions for positive class\n",
    "\n",
    "# Calculate evaluation metrics for Random Forest\n",
    "rf_accuracy = accuracy_score(y_test, y_pred_rf)  # Calculate accuracy\n",
    "rf_precision = precision_score(y_test, y_pred_rf, average='binary', zero_division=0)  # Calculate precision\n",
    "rf_recall = recall_score(y_test, y_pred_rf, average='binary', zero_division=0)  # Calculate recall\n",
    "rf_f1 = f1_score(y_test, y_pred_rf, average='binary', zero_division=0)  # Calculate F1-score\n",
    "\n",
    "print(f\"\\nRandom Forest Performance:\")  # Announce RF results\n",
    "print(f\"  Accuracy:  {rf_accuracy:.4f}\")  # Display accuracy\n",
    "print(f\"  Precision: {rf_precision:.4f}\")  # Display precision\n",
    "print(f\"  Recall:    {rf_recall:.4f}\")  # Display recall\n",
    "print(f\"  F1-Score:  {rf_f1:.4f}\")  # Display F1-score\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 5A: CROSS-VALIDATION FOR RANDOM FOREST\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nPerforming 5-Fold Cross-Validation for Random Forest...\")  # Announce CV\n",
    "rf_cv_scores = cross_val_score(rf_model, X_train_processed, y_train, cv=5, scoring='accuracy')  # Perform 5-fold CV\n",
    "print(f\"CV Accuracy Scores: {rf_cv_scores}\")  # Display individual fold scores\n",
    "print(f\"Mean CV Accuracy: {rf_cv_scores.mean():.4f} (+/- {rf_cv_scores.std() * 2:.4f})\")  # Display mean and std\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 5B: FEATURE IMPORTANCE FOR RANDOM FOREST\n",
    "# ============================================================================\n",
    "\n",
    "feature_importances = rf_model.feature_importances_  # Extract feature importances from trained model\n",
    "feature_names = numeric_features  # Use numeric feature names (categorical would be expanded)\n",
    "\n",
    "# If categorical features were one-hot encoded, get transformed feature names\n",
    "if len(categorical_features) > 0:  # Check if categorical features present\n",
    "    feature_names = preprocessor.get_feature_names_out()  # Get all transformed feature names\n",
    "\n",
    "# Sort features by importance\n",
    "indices = np.argsort(feature_importances)[::-1]  # Get indices sorted by importance (descending)\n",
    "top_n = min(15, len(feature_importances))  # Limit to top 15 features\n",
    "\n",
    "plt.figure(figsize=(10, 8))  # Create figure\n",
    "plt.barh(range(top_n), feature_importances[indices[:top_n]], color='teal', alpha=0.7)  # Create horizontal bar plot\n",
    "plt.yticks(range(top_n), [feature_names[i] for i in indices[:top_n]])  # Set y-axis labels\n",
    "plt.xlabel('Importance', fontsize=12)  # Label x-axis\n",
    "plt.ylabel('Features', fontsize=12)  # Label y-axis\n",
    "plt.title('Random Forest - Top Feature Importances', fontsize=14, fontweight='bold')  # Add title\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to show most important at top\n",
    "plt.grid(axis='x', alpha=0.3)  # Add vertical grid lines\n",
    "plt.tight_layout()  # Adjust layout\n",
    "plt.savefig('artifacts/figures/rf_feature_importance.png', dpi=300, bbox_inches='tight')  # Save figure\n",
    "plt.close()  # Close figure\n",
    "print(\"Random Forest feature importance plot saved\")  # Confirm save\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 5C: CONFUSION MATRIX FOR RANDOM FOREST\n",
    "# ============================================================================\n",
    "\n",
    "rf_cm = confusion_matrix(y_test, y_pred_rf)  # Calculate confusion matrix\n",
    "plt.figure(figsize=(8, 6))  # Create figure\n",
    "sns.heatmap(rf_cm, annot=True, fmt='d', cmap='Greens', cbar=True,  # Create heatmap\n",
    "            xticklabels=['No Disease', 'Disease'],  # Label x-axis\n",
    "            yticklabels=['No Disease', 'Disease'])  # Label y-axis\n",
    "plt.title('Random Forest - Confusion Matrix', fontsize=14, fontweight='bold')  # Add title\n",
    "plt.ylabel('Actual', fontsize=12)  # Label y-axis\n",
    "plt.xlabel('Predicted', fontsize=12)  # Label x-axis\n",
    "plt.tight_layout()  # Adjust layout\n",
    "plt.savefig('artifacts/figures/rf_confusion_matrix.png', dpi=300, bbox_inches='tight')  # Save figure\n",
    "plt.close()  # Close figure\n",
    "print(\"Random Forest confusion matrix saved\")  # Confirm save\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 5D: ROC CURVE FOR RANDOM FOREST\n",
    "# ============================================================================\n",
    "\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_proba_rf)  # Calculate ROC curve points\n",
    "roc_auc_rf = auc(fpr_rf, tpr_rf)  # Calculate area under ROC curve\n",
    "\n",
    "plt.figure(figsize=(8, 6))  # Create figure\n",
    "plt.plot(fpr_rf, tpr_rf, color='green', lw=2, label=f'ROC curve (AUC = {roc_auc_rf:.2f})')  # Plot ROC curve\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')  # Plot diagonal reference\n",
    "plt.xlim([0.0, 1.0])  # Set x-axis limits\n",
    "plt.ylim([0.0, 1.05])  # Set y-axis limits\n",
    "plt.xlabel('False Positive Rate', fontsize=12)  # Label x-axis\n",
    "plt.ylabel('True Positive Rate', fontsize=12)  # Label y-axis\n",
    "plt.title('Random Forest - ROC Curve', fontsize=14, fontweight='bold')  # Add title\n",
    "plt.legend(loc=\"lower right\")  # Add legend\n",
    "plt.grid(alpha=0.3)  # Add grid\n",
    "plt.tight_layout()  # Adjust layout\n",
    "plt.savefig('artifacts/figures/rf_roc_curve.png', dpi=300, bbox_inches='tight')  # Save figure\n",
    "plt.close()  # Close figure\n",
    "print(\"Random Forest ROC curve saved\")  # Confirm save\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 5E: PRECISION-RECALL CURVE FOR RANDOM FOREST\n",
    "# ============================================================================\n",
    "\n",
    "precision_rf, recall_rf, _ = precision_recall_curve(y_test, y_pred_proba_rf)  # Calculate PR curve points\n",
    "plt.figure(figsize=(8, 6))  # Create figure\n",
    "plt.plot(recall_rf, precision_rf, color='green', lw=2, label='PR curve')  # Plot PR curve\n",
    "plt.xlabel('Recall', fontsize=12)  # Label x-axis\n",
    "plt.ylabel('Precision', fontsize=12)  # Label y-axis\n",
    "plt.title('Random Forest - Precision-Recall Curve', fontsize=14, fontweight='bold')  # Add title\n",
    "plt.legend(loc=\"lower left\")  # Add legend\n",
    "plt.grid(alpha=0.3)  # Add grid\n",
    "plt.tight_layout()  # Adjust layout\n",
    "plt.savefig('artifacts/figures/rf_pr_curve.png', dpi=300, bbox_inches='tight')  # Save figure\n",
    "plt.close()  # Close figure\n",
    "print(\"Random Forest PR curve saved\")  # Confirm save\n",
    "\n",
    "# Save Random Forest model and metrics\n",
    "joblib.dump(rf_model, 'artifacts/models/random_forest_model.pkl')  # Save trained model\n",
    "rf_metrics = {  # Create metrics dictionary\n",
    "    'model': 'Random Forest',  # Model name\n",
    "    'accuracy': float(rf_accuracy),  # Convert to float for JSON serialization\n",
    "    'precision': float(rf_precision),  # Convert to float\n",
    "    'recall': float(rf_recall),  # Convert to float\n",
    "    'f1_score': float(rf_f1),  # Convert to float\n",
    "    'roc_auc': float(roc_auc_rf),  # Convert to float\n",
    "    'cv_mean': float(rf_cv_scores.mean()),  # Cross-validation mean\n",
    "    'cv_std': float(rf_cv_scores.std())  # Cross-validation standard deviation\n",
    "}\n",
    "with open('artifacts/metrics/random_forest_metrics.json', 'w') as f:  # Open file for writing\n",
    "    json.dump(rf_metrics, f, indent=4)  # Save metrics as formatted JSON\n",
    "print(\"Random Forest model and metrics saved\")  # Confirm save\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 8: SUMMARY AND COMPLETION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)  # Print separator line\n",
    "print(\"MEMBER 1 COMPLETION SUMMARY\")  # Print summary header\n",
    "print(\"=\"*80)  # Print separator line\n",
    "print(\"\\n Data preprocessing completed\")  # Confirm preprocessing\n",
    "print(\"Decision Tree trained and evaluated\")  # Confirm DT completion\n",
    "print(\"Random Forest trained and evaluated\")  # Confirm RF completion\n",
    "print(\"All models saved to artifacts/models/\")  # Confirm model saves\n",
    "print(\"All metrics saved to artifacts/metrics/\")  # Confirm metrics saves\n",
    "print(\"All figures saved to artifacts/figures/\")  # Confirm figure saves\n",
    "print(\"\\nReady for Member 2 to proceed with Naive Bayes and Deep Learning models.\")  # Announce readiness\n",
    "print(\"=\"*80)  # Print separator line"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
